{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa74496f-176b-4cf7-a72e-b42d1fe6167e",
   "metadata": {},
   "source": [
    "# 1. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd16c92-2a37-48c5-ba3b-13ce96bd3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8cb4f-1fca-4e91-a307-7816c526e91e",
   "metadata": {},
   "source": [
    "# 2. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ee8048-13d2-4755-a172-a585801833ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/vv093/Downloads/archive (1)/train/\"\n",
    "X, y = [], []\n",
    "for label in os.listdir(data_dir):\n",
    "    folder = os.path.join(data_dir, label)\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder, file)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            resized = cv2.resize(img, (48, 48))\n",
    "            X.append(resized)\n",
    "            y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f45f97-bdf1-4ab7-82d5-aec5c63e361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Folder 'angry': 3995 images\n",
      " Folder 'disgust': 436 images\n",
      " Folder 'fear': 4097 images\n",
      " Folder 'happy': 7215 images\n",
      " Folder 'neutral': 4965 images\n",
      " Folder 'sad': 4830 images\n",
      " Folder 'surprise': 3171 images\n",
      "\n",
      " Total images in dataset: 28709\n"
     ]
    }
   ],
   "source": [
    "#Find the how many Train data Images is there\n",
    "total_images = 0  \n",
    "for emotion_folder in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, emotion_folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        image_files = [f for f in os.listdir(folder_path)\n",
    "                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        count = len(image_files)\n",
    "        total_images += count\n",
    "        print(f\" Folder '{emotion_folder}': {count} images\")\n",
    "print(f\"\\n Total images in dataset: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d5931-f340-45e1-8914-aab33bd42554",
   "metadata": {},
   "source": [
    "# 3.Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ef4578-2d8f-4210-8f45-8c22e29c473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(-1, 48, 48, 1) / 255.0\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2754765-4ae5-4a13-acfb-a24811b8223f",
   "metadata": {},
   "source": [
    "# 4. Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc0d3b5e-61d7-4e69-a703-cecc20639bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2cf6f0-7fd2-46b9-b759-595fe81a6682",
   "metadata": {},
   "source": [
    "## 5. Built a CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f3aed6-fc8b-4a58-9c71-557846572f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "model = Sequential([\n",
    "    Input(shape=(48,48,1)),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(y.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6cd8c-08cc-4805-8f77-28b812ab3866",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a0ce17f-f1d4-4b79-811f-7eceb740b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab0e1c-3b2f-4df7-9173-457eadb08342",
   "metadata": {},
   "source": [
    "# 5.Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ebc5521-5097-49e8-91a7-0ef633143aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.6411 - loss: 0.9622 - val_accuracy: 0.5686 - val_loss: 1.1346\n",
      "Epoch 2/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 115ms/step - accuracy: 0.6578 - loss: 0.9186 - val_accuracy: 0.5674 - val_loss: 1.1550\n",
      "Epoch 3/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 112ms/step - accuracy: 0.6533 - loss: 0.9084 - val_accuracy: 0.5723 - val_loss: 1.1469\n",
      "Epoch 4/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 111ms/step - accuracy: 0.6663 - loss: 0.8833 - val_accuracy: 0.5751 - val_loss: 1.1526\n",
      "Epoch 5/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 112ms/step - accuracy: 0.6769 - loss: 0.8596 - val_accuracy: 0.5737 - val_loss: 1.1510\n",
      "Epoch 6/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.6972 - loss: 0.8285 - val_accuracy: 0.5801 - val_loss: 1.1653\n",
      "Epoch 7/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 107ms/step - accuracy: 0.6985 - loss: 0.8167 - val_accuracy: 0.5731 - val_loss: 1.1636\n",
      "Epoch 8/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 103ms/step - accuracy: 0.7052 - loss: 0.7868 - val_accuracy: 0.5770 - val_loss: 1.1766\n",
      "Epoch 9/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 118ms/step - accuracy: 0.7159 - loss: 0.7750 - val_accuracy: 0.5810 - val_loss: 1.1648\n",
      "Epoch 10/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 116ms/step - accuracy: 0.7291 - loss: 0.7369 - val_accuracy: 0.5745 - val_loss: 1.2105\n",
      "Epoch 11/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 115ms/step - accuracy: 0.7358 - loss: 0.7115 - val_accuracy: 0.5733 - val_loss: 1.1935\n",
      "Epoch 12/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 114ms/step - accuracy: 0.7431 - loss: 0.6970 - val_accuracy: 0.5752 - val_loss: 1.1905\n",
      "Epoch 13/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 103ms/step - accuracy: 0.7460 - loss: 0.6781 - val_accuracy: 0.5832 - val_loss: 1.2028\n",
      "Epoch 14/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 110ms/step - accuracy: 0.7532 - loss: 0.6647 - val_accuracy: 0.5761 - val_loss: 1.2121\n",
      "Epoch 15/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 111ms/step - accuracy: 0.7609 - loss: 0.6474 - val_accuracy: 0.5796 - val_loss: 1.2315\n",
      "Epoch 16/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.7749 - loss: 0.6116 - val_accuracy: 0.5794 - val_loss: 1.2357\n",
      "Epoch 17/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 111ms/step - accuracy: 0.7785 - loss: 0.6014 - val_accuracy: 0.5799 - val_loss: 1.2521\n",
      "Epoch 18/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 107ms/step - accuracy: 0.7817 - loss: 0.5964 - val_accuracy: 0.5806 - val_loss: 1.2553\n",
      "Epoch 19/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 111ms/step - accuracy: 0.7904 - loss: 0.5689 - val_accuracy: 0.5822 - val_loss: 1.2583\n",
      "Epoch 20/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 111ms/step - accuracy: 0.7874 - loss: 0.5717 - val_accuracy: 0.5761 - val_loss: 1.2669\n",
      "Epoch 21/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 107ms/step - accuracy: 0.7990 - loss: 0.5584 - val_accuracy: 0.5730 - val_loss: 1.3015\n",
      "Epoch 22/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 112ms/step - accuracy: 0.8000 - loss: 0.5448 - val_accuracy: 0.5839 - val_loss: 1.3199\n",
      "Epoch 23/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.7974 - loss: 0.5382 - val_accuracy: 0.5763 - val_loss: 1.3718\n",
      "Epoch 24/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 81ms/step - accuracy: 0.8083 - loss: 0.5164 - val_accuracy: 0.5761 - val_loss: 1.3451\n",
      "Epoch 25/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 85ms/step - accuracy: 0.8118 - loss: 0.5083 - val_accuracy: 0.5756 - val_loss: 1.3490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x299c2697190>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57d2f5fa-0178-4d1f-bf7c-68732b1d4f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.5800 - loss: 1.3247\n",
      "\n",
      " Final Test Accuracy: 57.56%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"\\n Final Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530da347-9326-4d2d-bd68-71ced0b527b9",
   "metadata": {},
   "source": [
    "# 6.Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cee620aa-e29f-43df-9f04-e27de803b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"model\", exist_ok=True)\n",
    "model.save(\"model/emotion_model.keras\")\n",
    "os.makedirs(\"Classes\", exist_ok=True)\n",
    "np.save(\"Classes/classes.npy\", le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e743a3-522d-4682-b466-987842a0a8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
